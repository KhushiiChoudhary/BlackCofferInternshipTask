{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTZAInvSZXPQ1WEfG262+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhushiiChoudhary/BlackCofferInternshipTask/blob/main/BlackCoffer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2zSbKGTGLo1",
        "outputId": "021cb1e1-8be6-4af2-9f70-8ec00022ce84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article text extracted and saved into 'Extracted_Text/blackassign0001.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0002.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0003.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0004.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0005.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0006.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0007.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0008.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0009.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0010.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0011.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0012.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0013.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0014.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0015.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0016.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0017.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0018.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0019.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0020.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0021.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0022.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0023.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0024.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0025.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0026.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0027.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0028.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0029.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0030.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0031.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0032.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0033.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0034.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0035.txt' file.\n",
            "Error: Couldn't find article content on the page: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0037.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0038.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0039.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0040.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0041.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0042.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0043.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0044.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0045.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0046.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0047.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0048.txt' file.\n",
            "Error: Couldn't find article content on the page: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0050.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0051.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0052.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0053.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0054.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0055.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0056.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0057.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0058.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0059.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0060.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0061.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0062.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0063.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0064.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0065.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0066.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0067.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0068.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0069.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0070.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0071.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0072.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0073.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0074.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0075.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0076.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0077.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0078.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0079.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0080.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0081.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0082.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0083.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0084.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0085.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0086.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0087.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0088.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0089.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0090.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0091.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0092.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0093.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0094.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0095.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0096.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0097.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0098.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0099.txt' file.\n",
            "Article text extracted and saved into 'Extracted_Text/blackassign0100.txt' file.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Function to extract article text from a URL\n",
        "def extract_article_text(url):\n",
        "    # Make a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Find the main article content\n",
        "    article_content = soup.find('div', class_='td-post-content')\n",
        "    if article_content:\n",
        "        # Extract text from paragraphs within the main content\n",
        "        paragraphs = article_content.find_all('p')\n",
        "        article_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return article_text.strip()\n",
        "    else:\n",
        "        print(f\"Error: Couldn't find article content on the page: {url}\")\n",
        "        return None\n",
        "\n",
        "# Read input Excel file containing URLs\n",
        "input_df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Create a directory to save extracted text files\n",
        "if not os.path.exists('Extracted_Text'):\n",
        "    os.makedirs('Extracted_Text')\n",
        "\n",
        "# Iterate through each URL\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    # Extract article text\n",
        "    article_text = extract_article_text(url)\n",
        "    if article_text:\n",
        "        # Save extracted text into a text file\n",
        "        with open(f'Extracted_Text/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write(article_text)\n",
        "        print(f\"Article text extracted and saved into 'Extracted_Text/{url_id}.txt' file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to compute variables from article text\n",
        "def compute_variables(article_text):\n",
        "    # Tokenize the text into words and sentences\n",
        "    words = word_tokenize(article_text)\n",
        "    sentences = sent_tokenize(article_text)\n",
        "\n",
        "    # Compute variables\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_length = word_count / sentence_count\n",
        "    # You can compute other variables here\n",
        "\n",
        "    return {\n",
        "        'WORD COUNT': word_count,\n",
        "        'SENTENCE COUNT': sentence_count,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "        # Add other variables here\n",
        "    }\n",
        "\n",
        "# Read the list of extracted text files\n",
        "extracted_files = os.listdir('Extracted_Text')\n",
        "\n",
        "# Create an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Iterate through each extracted text file\n",
        "for file_name in extracted_files:\n",
        "    url_id = file_name.split('.')[0]  # Extract URL_ID from the file name\n",
        "    # Read the content of the text file\n",
        "    with open(f'Extracted_Text/{file_name}', 'r', encoding='utf-8') as file:\n",
        "        article_text = file.read()\n",
        "    # Compute variables\n",
        "    variables = compute_variables(article_text)\n",
        "    # Store results\n",
        "    result = {\n",
        "        'URL_ID': url_id,\n",
        "        **variables\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "# Create DataFrame from results\n",
        "output_df = pd.DataFrame(results)\n",
        "\n",
        "# Save output DataFrame to Excel file\n",
        "output_df.to_excel('Output.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT6LX6RpHqhM",
        "outputId": "79f6f368-04a7-4b27-ae4b-c132c82210e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install syllapy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tt_y9yxIdI5",
        "outputId": "95b0938d-12be-412b-9ea0-ee643740d263"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: syllapy in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "\n",
        "def syllable_count(word):\n",
        "    try:\n",
        "        pronunciation = nltk.corpus.cmudict.dict()[word.lower()][0]\n",
        "        return sum(1 for phoneme in pronunciation if phoneme[-1].isdigit())\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "# Test the syllable_count function\n",
        "words = ['hello', 'world', 'programming', 'artificial', 'intelligence']\n",
        "for word in words:\n",
        "    print(f'{word}: {syllable_count(word)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK2GcOVyIUvG",
        "outputId": "eb9081ff-7bd6-4a7a-f12b-0b16df6bb2b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello: 2\n",
            "world: 1\n",
            "programming: 3\n",
            "artificial: 4\n",
            "intelligence: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "def compute_additional_variables(article_text):\n",
        "    words = word_tokenize(article_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    word_count = len(filtered_words)\n",
        "    sentence_count = len(sent_tokenize(article_text))\n",
        "    avg_sentence_length = word_count / sentence_count\n",
        "\n",
        "    syllables_per_word = sum([syllable_count(word) for word in filtered_words]) / word_count\n",
        "\n",
        "    percentage_complex_words = len([word for word in filtered_words if syllable_count(word) > 2]) / word_count * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    return {\n",
        "        'WORD COUNT': word_count,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        # Add other variables here\n",
        "    }\n",
        "\n",
        "# Test the compute_additional_variables function with a sample article text\n",
        "article_text = \"\"\"\n",
        "    Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
        "\"\"\"\n",
        "additional_variables = compute_additional_variables(article_text)\n",
        "print(additional_variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvQlX2QWId6e",
        "outputId": "ea322911-9a4b-4677-8e3d-277c7b847acc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'WORD COUNT': 54, 'AVG SENTENCE LENGTH': 27.0, 'PERCENTAGE OF COMPLEX WORDS': 46.2962962962963, 'FOG INDEX': 29.318518518518523, 'SYLLABLE PER WORD': 2.074074074074074}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to compute variables from article text\n",
        "def compute_variables(article_text):\n",
        "    # Tokenize the text into words and sentences\n",
        "    words = word_tokenize(article_text)\n",
        "    sentences = sent_tokenize(article_text)\n",
        "\n",
        "    # Compute variables\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_length = word_count / sentence_count\n",
        "\n",
        "    return {\n",
        "        'WORD COUNT': word_count,\n",
        "        'SENTENCE COUNT': sentence_count,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "    }\n",
        "\n",
        "# Function to compute additional variables from article text\n",
        "def compute_additional_variables(article_text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(article_text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Compute additional variables\n",
        "    word_count = len(filtered_words)\n",
        "    sentence_count = len(sent_tokenize(article_text))\n",
        "    avg_sentence_length = word_count / sentence_count\n",
        "\n",
        "    # Compute POSITIVE SCORE, NEGATIVE SCORE, POLARITY SCORE, and SUBJECTIVITY SCORE using TextBlob\n",
        "    blob = TextBlob(article_text)\n",
        "    positive_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity > 0)\n",
        "    negative_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity < 0)\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "    # Compute AVG NUMBER OF WORDS PER SENTENCE\n",
        "    avg_words_per_sentence = word_count / sentence_count\n",
        "\n",
        "    # Count personal pronouns\n",
        "    personal_pronouns = sum(1 for word in filtered_words if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'we', 'us', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves'])\n",
        "\n",
        "    # Compute AVG WORD LENGTH\n",
        "    avg_word_length = sum(len(word) for word in filtered_words) / word_count\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': avg_word_length\n",
        "    }\n",
        "\n",
        "# Read the list of extracted text files\n",
        "extracted_files = os.listdir('Extracted_Text')\n",
        "\n",
        "# Create an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Iterate through each extracted text file\n",
        "for file_name in extracted_files:\n",
        "    url_id = file_name.split('.')[0]  # Extract URL_ID from the file name\n",
        "    # Read the content of the text file\n",
        "    with open(f'Extracted_Text/{file_name}', 'r', encoding='utf-8') as file:\n",
        "        article_text = file.read()\n",
        "    # Compute variables\n",
        "    variables = compute_variables(article_text)\n",
        "    # Compute additional variables\n",
        "    additional_variables = compute_additional_variables(article_text)\n",
        "    # Merge variables\n",
        "    result = {\n",
        "        'URL_ID': url_id,\n",
        "        **variables,\n",
        "        **additional_variables\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "# Create DataFrame from results\n",
        "output_df = pd.DataFrame(results)\n",
        "\n",
        "# Save output DataFrame to Excel file\n",
        "output_df.to_excel('Output.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPwcTatmNd4c",
        "outputId": "7082c1da-25c8-40f9-e6b6-2c1c5dc33a33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}